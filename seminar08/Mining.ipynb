{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data minining with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обычно очень много винимания в процессе обученя анализу данных уделяется алгоритмам и методам обработки данных и намного меньше внимания уделяется тому эти данные взять. При этом: <br>\n",
    "- Без самих данных навряд ли что-то получится; <br>\n",
    "- От качества данных зависит результат работы любого алогоритма; <br>\n",
    "- Часто работают лучше те модели которые обучались на более качественных или большем объеме данных.\n",
    "\n",
    "### Где брать данные?\n",
    "\n",
    "Конечно, данные вам могут предоставить на работе или заказчик. Вы можете работать в одной из компаний где данных больше чем предостаточно. А что если их не достаточно или в них нет того что вам нужно.\n",
    "\n",
    "\n",
    "### В этом случае лучше всего действовать в следующей последовательности:\n",
    "\n",
    "- Найти готовый датасет;\n",
    "- Собрать данные самому:\n",
    "   - для этого можно воспользоваться API;\n",
    "   - собрать даные со страниц сайтов.\n",
    "   \n",
    "Существует несколько инструментов для парсинга данных самостоятельно но чаще всего используют:\n",
    "- библиотеку requests + библиотеку beautiful soup\n",
    "- фремйворк Scrappy \n",
    "\n",
    "Оба варианта взаимоисключающие и необходимость использовать тот или иной зависит от случая, подробнее об этом будет ниже.\n",
    "\n",
    "\n",
    "### Поиск готового датасета.\n",
    "Логичнее всего начать с поиска готового датасета. Существует много сайтов где можно найти уже готовые данные, например, очень много данных есть на платформе <a href=\"https://www.kaggle.com/datasets\">Kaggle<a/>. Менее популярная платформа, но не менее интересная <a href=\"http://academictorrents.com/browse.php\">academictorrents.com </a>, советую проверить. <br>\n",
    "Существуют и еще менее тривиальные источники данных о существовании которых иногда трудно догодаться. Так например, есть Google Big Querry на котором сейчас выложено около 40 публичных датасетов, один из них это все записи форума reddit. Казалось бы зачем нам форум такой форум? На самом деле применений может быть очень много, например, по его данным легко обучить классификатор текстов на разные категории, а категорий на форуме очень много.  <br>\n",
    "    \n",
    "Интерфейс у Google Big Querry это SQL, что очень удобно и можно сразу вытащить то что нужно. Попробовать можно здесь: <a href=\"https://bigquery.cloud.google.com/results/bigdatagame-200413:US.bquijob_609efa7_16871e6cd5e?pli=1\">Google Big Querry</a> <br>\n",
    "Пример запроса который достает 1500 комментариве из раздела \"Art\" c лайками больше 5ти. <br>\n",
    "    \n",
    "<strong><i>SELECT body, score FROM <br>\n",
    "[fh-bigquery:reddit_comments.2017_05], <br>\n",
    "WHERE subreddit LIKE 'Art' AND score > 5 <br>\n",
    "LIMIT 15000 <br></i></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самостоятельный сбор данных с помощью API.\n",
    "Ок, если не удалось найти готовый датасет, то возможно они есть в отрытом доступе на каком-то сайте или платформе через общедоступный API. <br>\n",
    " <br>\n",
    "#### Работа с API выглядит следующим образом. \n",
    "Есть два метода (на самом деле их больше) post и get с помощью которых мы можем обратиться к сайту или платформе. С помощью метода get мы можем можем сделать запрос ( напрмер когда мы открываем сайт по ссылке то это get), а с помощью метода post мы можем передать какеи-то значения клиенту.<br>\n",
    "Так например, сайт HH.ru предоставляет свой API и по следующему запросу мы можем получить список вакансий определенной категории 1.221 (цифры обозначающие вакансии это их внутренняя кухня, тут без лишних деталей). <br>\n",
    "https://api.hh.ru/vacancies?specialization=1.221 <br>\n",
    "В ответ на странице вы увидите скорее всего Json фаил, который очень легко читать и с которым приятно работать. <br>\n",
    "Теперь нам нужно понять что с этим делать дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с библиотекой requests\n",
    "Собственно для работы с post и get запросами нам и понадобиться библиотека requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:06:48.895754Z",
     "start_time": "2019-04-12T23:06:48.891934Z"
    }
   },
   "source": [
    "Для установки библиотеки используйте следующую комманду в терминале (если на Linux)\n",
    "\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Документация по библиотеке requests: http://docs.python-requests.org/en/master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на примере сайта HH.ru как работать с библиотекой и как можно получить какие-то данные с помощью API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:41.685819Z",
     "start_time": "2019-04-12T23:10:41.318364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url1 = 'https://api.hh.ru/vacancies?specialization=1.221'\n",
    "url2 = 'https://api.hh.ru/vacancies/1'\n",
    "resp = requests.get(url2)\n",
    "print(resp)\n",
    "resp = requests.get(url1)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мы видим что для одной из ссылок клиент вернул статус 404, а для другой 200.\n",
    "\n",
    "Это код ответа от клиента, код 200 обозначает что все ок, а код 404 обозначает Not found. Полный списко кодов состояния HTTP можно посмотреть <a href=\"https://ru.wikipedia.org/wiki/Список_кодов_состояния_HTTPhttps://ru.wikipedia.org/wiki/Список_кодов_состояния_HTTP\"> вот здесь. </a> В дальнейшем, в рамках этого туториала нас интересует только код 200. Но на практике полезно знать и другие. <br>\n",
    "И так, для одной из ссылок мы получили 200, все ок, двигаемся дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:42.881826Z",
     "start_time": "2019-04-12T23:10:42.755666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url=url1+'&page=1&per_page100'\n",
    "resp = requests.get(url)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'&page=1&per_page100'  - это опять внутренняя кухня API HH.ru, у всех платформ она разная. Здесь мы говорим что мы загружаем первую страницу и 100 результатов на странице. Почему нельзя сделать больше или подругому за раз зависит от того как это решили разработчики API. <br>\n",
    "Но можно получить больше результатов следующим образом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.215862Z",
     "start_time": "2019-04-12T23:10:43.831816Z"
    }
   },
   "outputs": [],
   "source": [
    "searches = []\n",
    "for page in range(20):\n",
    "    searches.append(\n",
    "        requests.get(url1 + '&page={}&per_page=100'.format(page)).json()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.228873Z",
     "start_time": "2019-04-12T23:10:49.218036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.hh.ru/vacancies/29693966?host=hh.ru'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Можно открыть первую ссылку в нашем дасатае и посмореть что мы собрали.\n",
    "searches[0]['items'][0]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.238614Z",
     "start_time": "2019-04-12T23:10:49.231597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '29693966',\n",
       " 'premium': False,\n",
       " 'name': 'Senior Android developer',\n",
       " 'department': None,\n",
       " 'has_test': False,\n",
       " 'response_letter_required': False,\n",
       " 'area': {'id': '26', 'name': 'Воронеж', 'url': 'https://api.hh.ru/areas/26'},\n",
       " 'salary': {'from': 120000, 'to': 150000, 'currency': 'RUR', 'gross': False},\n",
       " 'type': {'id': 'open', 'name': 'Открытая'},\n",
       " 'address': None,\n",
       " 'response_url': None,\n",
       " 'sort_point_distance': None,\n",
       " 'employer': {'id': '1321508',\n",
       "  'name': 'AdCombo.com',\n",
       "  'url': 'https://api.hh.ru/employers/1321508',\n",
       "  'alternate_url': 'https://hh.ru/employer/1321508',\n",
       "  'logo_urls': {'original': 'https://hhcdn.ru/employer-logo-original/259233.png',\n",
       "   '90': 'https://hhcdn.ru/employer-logo/1479462.png',\n",
       "   '240': 'https://hhcdn.ru/employer-logo/1479463.png'},\n",
       "  'vacancies_url': 'https://api.hh.ru/vacancies?employer_id=1321508',\n",
       "  'trusted': True},\n",
       " 'published_at': '2019-04-13T01:24:15+0300',\n",
       " 'created_at': '2019-04-13T01:24:15+0300',\n",
       " 'archived': False,\n",
       " 'apply_alternate_url': 'https://hh.ru/applicant/vacancy_response?vacancyId=29693966',\n",
       " 'insider_interview': None,\n",
       " 'url': 'https://api.hh.ru/vacancies/29693966?host=hh.ru',\n",
       " 'alternate_url': 'https://hh.ru/vacancy/29693966',\n",
       " 'relations': [],\n",
       " 'snippet': {'requirement': 'Знание Kotlin. Опыт работы с Firebase, Appsflyer, AppMetrica или другими. Опыт разработки приложений под Android от 3 лет. ',\n",
       "  'responsibility': 'Разработка и поддержка приложений под ОС Android (только внутренние проекты компании, без внешних заказчиков). Взаимодействие с другими отделами (дизайнер, тестировщик...'},\n",
       " 'contacts': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacs = [x['items'] for x in searches]\n",
    "vacs = sum(list(vacs),[])\n",
    "vacs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.249511Z",
     "start_time": "2019-04-12T23:10:49.242210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senior android developer',\n",
       " 'senior ios developer',\n",
       " 'web-дизайнер',\n",
       " 'junior backend php developer',\n",
       " 'middle backend php developer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Посморим на первые 5 названий вакансий\n",
    "titles = [item['name'].lower() for item in vacs]\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.293857Z",
     "start_time": "2019-04-12T23:10:49.251266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'программист 1с'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#А теперь очень просто найдем самое часто встречаемое название вакансии\n",
    "\n",
    "max(set(titles), key=titles.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать API можно не только для текстовой информации, хотя чаще всего для нее. Вот пример работы c API Flickr https://gist.github.com/diff7/627a306d6d8e4ecd0162808deae7d244 для получения картинок. Для использования нужно импортировать API key на сайте и установить пакеты, которые импортируются в начале скрипта, если они еще не стоят. <br>\n",
    "\n",
    "\n",
    "А если вам вдруг понравилось работать с апишками то вот здесь есть очень хороший проект с кучей всяких разных API со всего интернета.\n",
    "<a href=\"https://public-apis.xyz/\">public-apis.xyz</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Beautiful Soup\n",
    "Документация : https://www.crummy.com/software/BeautifulSoup/bs4/doc/ <br>\n",
    "\n",
    "Чаще всего через API можно получить намного больше интересной и качественной информации, убедиться в этом можно опять же зайдя на сайт public-apis.xyz там есть и статистика из онлайн игр отзывы по фильмам и много всего. Но  иногда сайты не предоставляют API, но данные на них все таки есть. (Мы не обсуждаем можно ли их использовать или нет, скорее всего об этом написано на самом сайте) Тогда приходиться дейстовать подругому. <br>\n",
    "Посмотрим на примере сайта http://quotes.toscrape.com который был создан для как раз для практики данных навыков. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:49.403044Z",
     "start_time": "2019-04-12T23:10:49.296203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"col-md-8\">\\n\\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\\n        <span class=\"text\" itemprop=\"text\">\\xe2\\x80\\x9cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\\xe2\\x80\\x9d</span>\\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\\n        <a href=\"/author/Albert-Einstein\">(about)</a>\\n      '\n"
     ]
    }
   ],
   "source": [
    "link='http://quotes.toscrape.com'\n",
    "page=requests.get(link)\n",
    "if page.status_code == 200:  \n",
    "    print(page.content[700:1100]) #Посмотрим на то что мы получили между 800 и 1100 набором символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно здесь присутствует какой - то текст, но ответ от страницы нам пришел так как видит ее браузер, т.е. с HTML тегами, сss и возможно JavaScript. Собственно что бы очистить страницу от HTML и всего прочего нам и нужна библиотека Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чуть - чуть про HTML, если вы в курсе то пропустите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>  \n",
    "<html>  \n",
    "    <head  style=\"color:red\">Это простой пример HTML разметки используемый для форматирования текста.</head>\n",
    "    <div> Если нажаит на <strong>этот текст</strong> два раза, то вы увидите его так как его видит браузер.</div>\n",
    "    <body>\n",
    "        <h1><span class=\"cm-string\"> Заголовок &lt;h1&gt; </span></h1>\n",
    "        <p>Параграф &lt;p&gt; </p>\n",
    "        <a href=\"www.nowhere.space\">Ссылка в никуда</a>\n",
    "        <div>блок &lt;div&gt; </div>\n",
    "        <h2  style=\"color:red\" > И конечно картинка: </h2>\n",
    "        <img src=\"https://cs6.pikabu.ru/post_img/big/2015/06/18/3/1434596941_632146314.jpg\">\n",
    "        <p style=\"color:blue;\">Для того что бы просмотреть страницу в HTML можно нажать правой кнопкой мыши и выбрать \"inspect\" в появившимся меню.</p>\n",
    "        <p>В общем, функционал зависит от браузера, но в целом должно появится что-то вроде этого. Можете попробовать с сайтом\n",
    "            <a href=\"http://quotes.toscrape.com\"> http://quotes.toscrape.com </a></p>\n",
    "        <img src=\"https://mydatacareer.com/wp-content/uploads/2017/05/scraping3.png\">\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь мы знаем каким элементом страницы является тот или иной текст и сможем легко его извлечь с помощью библиотеки Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для установки библиотеки используйте следующую комманду в терминале (если на Linux)\n",
    "\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:50.790405Z",
     "start_time": "2019-04-12T23:10:50.571620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“I have not failed. I've just found 10,000 ways that won't work.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it's in hot water.”</span>,\n",
       " <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "\n",
    "link='http://quotes.toscrape.com'\n",
    "page=requests.get(link)\n",
    "if page.status_code == 200:  \n",
    "    soup = bs.BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "soup.find_all('span', attrs={'class': 'text'}) #Как видно из картинки выше, наша цитата находится внутри тега span c классом text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:51.086412Z",
     "start_time": "2019-04-12T23:10:51.080141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags=soup.find_all('span', attrs={'class': 'text'})\n",
    "tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:10:51.722254Z",
     "start_time": "2019-04-12T23:10:51.718052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
       " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
       " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
       " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
       " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
       " '“Try not to become a man of success. Rather become a man of value.”',\n",
       " '“It is better to be hated for what you are than to be loved for what you are not.”',\n",
       " \"“I have not failed. I've just found 10,000 ways that won't work.”\",\n",
       " \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\",\n",
       " '“A day without sunshine is like, you know, night.”']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.text for item in tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Недостатки такого метода\n",
    "\n",
    "С помощью комбинации Beautiful Soup + Requests можно получить данные с большого числа сайтов. Но существует ряд недостаткво.\n",
    "\n",
    "- Достаточно долго, так как процесс не параллелиться;\n",
    "- Для каждого нового сайта приходится писать все заново;\n",
    "- Некоторые сайты блокируют возможность парсинга, а некоторые используют JavaScript для отрисовки вместо HTML, тогда связки Beautiful Soup + Requests будет недостаточно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "Scrapy это уже не библиотека, а полноценный фреймворк для парсинга данных с сайтов с кучей настроек и шикарным функционалом.\n",
    "Основным недостатком Scrapy является достаточно высокий порог входа, придеться потратить некоторое время что бы разобраться во всех деталях. По дизайну фреймворк часто сравнивают с Django.\n",
    "\n",
    "Scrapy лучше использовать в следующих случаях:\n",
    "- Кодга важна скорость скачивания;\n",
    "- Когда данные парсятся с большого числа однотипных сайтов;\n",
    "- Когда предстоит парсить данные с одого или нескольких сайтов на регулярной основе;\n",
    "- Когда нужно спарсить очень большой сайт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа со Scrapy на примере сайте quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:08:45.222130Z",
     "start_time": "2019-04-12T23:08:41.493Z"
    }
   },
   "source": [
    "В командной строке\n",
    "\n",
    "pip install scrapy\n",
    "\n",
    "дальнейшие команды нужно делать в системном питоне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:08:45.224557Z",
     "start_time": "2019-04-12T23:08:41.510Z"
    }
   },
   "outputs": [],
   "source": [
    "#запустить Scrapy проект \n",
    "scrapy startproject proj\n",
    "\n",
    "#в директории проекта должна появится папка spiders\n",
    "cd scrapy/proj/proj/spiders \n",
    "\n",
    "#cоздайте в директории spiders фаил quotes_spider.py\n",
    "nano quotes_spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:09:42.317499Z",
     "start_time": "2019-04-12T23:09:41.832913Z"
    }
   },
   "outputs": [],
   "source": [
    "#содержимым файла будет соледующий код\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpiderSpider(scrapy.Spider):\n",
    "    name = 'quotes_spider' #название паука\n",
    "    allowed_domains = ['quotes.toscrape.com'] \n",
    "    start_urls = ['http://quotes.toscrape.com/'] #ссылка с которой начинается парсинг\n",
    "\n",
    "    def parse(self, response):\n",
    "        #для выделения HTML тегов или сss декораторов Scrapy использует Xpath\n",
    "        #по сути здесь мы передаем теже параметры которые передавали в Beautiful Soup\n",
    "        quotes = response.xpath(\"//div[@class='quote']//span[@class='text']/text()\").extract()\n",
    "        yield {'quotes': quotes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:09:44.363764Z",
     "start_time": "2019-04-12T23:09:44.359695Z"
    }
   },
   "outputs": [],
   "source": [
    "#После этого можно запустить Scrapy со следующей командой\n",
    "scrapy crawl quotes_spider -o quotes.csv\n",
    "\n",
    "#После исполнения в вашей директории должен появнится фаил quotes.csv \n",
    "#А ниже представлен примерный лог выполненной работы вашего паука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-12T23:09:45.917275Z",
     "start_time": "2019-04-12T23:09:45.910897Z"
    }
   },
   "outputs": [],
   "source": [
    "{'downloader/request_bytes': 446,\n",
    " 'downloader/request_count': 2\n",
    " 'downloader/request_method_count/GET': 2,\n",
    " 'downloader/response_bytes': 2701,\n",
    " 'downloader/response_count': 2,\n",
    " 'downloader/response_status_count/200': 1,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2019, 1, 22, 21, 36, 6, 198926),\n",
    " 'item_scraped_count': 1,\n",
    " 'log_count/DEBUG': 4,\n",
    " 'log_count/INFO': 8,\n",
    " 'memusage/max': 53608448,\n",
    " 'memusage/startup': 53608448,\n",
    " 'response_received_count': 2,\n",
    " 'scheduler/dequeued': 1,\n",
    " 'scheduler/dequeued/memory': 1,\n",
    " 'scheduler/enqueued': 1,\n",
    " 'scheduler/enqueued/memory': 1,\n",
    " 'start_time': datetime.datetime(2019, 1, 22, 21, 36, 5, 559859)}\n",
    "2019-01-23 00:36:06 [scrapy.core.engine] INFO: Spider closed (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один момент который хочеться упомямунть это то что Scrapy имеет удобный shell интерфейс для настройки пауков. По факту это обычный Ipython, но с набором внутренних команд Scrapy.\n",
    "\n",
    "Shell запускается командой scrapy shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmia8",
   "language": "python",
   "name": "dmia8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
